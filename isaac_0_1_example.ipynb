{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Isaac-0.1 FiftyOne Integration Example\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/isaac0_1/blob/main/isaac_0_1_example.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use Isaac-0.1 by Perceptron AI with FiftyOne for various computer vision tasks including object detection, OCR, classification, and visual question answering.\n",
        "\n",
        "## About Isaac-0.1\n",
        "\n",
        "Isaac-0.1 is an open-source, 2B-parameter perceptive-language model designed for real-world visual understanding tasks. It delivers capabilities comparable to models 50x larger while being efficient enough for practical applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "First, let's install the required dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q fiftyone\n",
        "%pip install -q perceptron\n",
        "%pip install -q transformers\n",
        "%pip install -q torch torchvision\n",
        "%pip install -q huggingface-hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Let's import the necessary libraries and suppress warnings for cleaner output:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "from transformers import logging\n",
        "\n",
        "# Suppress transformers warnings for cleaner output\n",
        "logging.set_verbosity_error()\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.utils.huggingface as fouh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register and Load Isaac-0.1 Model\n",
        "\n",
        "First, we need to register the Isaac-0.1 model zoo source and then load the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register the Isaac-0.1 model zoo source\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/isaac0_1\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "# Load the Isaac-0.1 model\n",
        "print(\"Loading Isaac-0.1 model...\")\n",
        "model = foz.load_zoo_model(\"PerceptronAI/Isaac-0.1\")\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Testing on Generic Images\n",
        "\n",
        "Let's load a sample dataset and test various operations on generic images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample dataset from Hugging Face\n",
        "dataset = fouh.load_from_hub(\n",
        "    \"Voxel51/GQA-Scene-Graph\",\n",
        "    max_samples=50,  # Using fewer samples for demo\n",
        ")\n",
        "\n",
        "print(f\"Loaded {len(dataset)} samples\")\n",
        "print(f\"First sample fields: {dataset.first().field_names}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract unique object labels from the dataset\n",
        "sample_objects = dataset.values(\"detections.detections.label\")\n",
        "sample_level_objects = [list(set(obj)) if obj else [] for obj in sample_objects]\n",
        "dataset.set_values(\"sample_level_objects\", sample_level_objects)\n",
        "\n",
        "# Display a sample of objects found\n",
        "print(\"Sample objects found in first image:\", sample_level_objects[0][:10] if sample_level_objects[0] else \"None\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Visual Question Answering (VQA)\n",
        "\n",
        "Let's use Isaac-0.1 to answer questions about the images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to VQA mode\n",
        "model.operation = \"vqa\"\n",
        "print(f\"System prompt for VQA:\\n{model.system_prompt}\\n\")\n",
        "\n",
        "# Set the question prompt\n",
        "model.prompt = \"Provide a short description of the spatial relationships between the objects in this scene\"\n",
        "\n",
        "# Apply the model to a subset of samples\n",
        "print(\"Running VQA on dataset...\")\n",
        "dataset.apply_model(model, label_field=\"vqa_description\")\n",
        "\n",
        "# Display results\n",
        "first_sample = dataset.first()\n",
        "print(f\"VQA Result for first image:\\n{first_sample.vqa_description}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Object Detection\n",
        "\n",
        "Now let's detect objects in the images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to detection mode\n",
        "model.operation = \"detect\"\n",
        "print(f\"System prompt for detection:\\n{model.system_prompt[:200]}...\\n\")\n",
        "\n",
        "# Use sample-level prompts for detection (objects from the ground truth)\n",
        "print(\"Running object detection using sample-level prompts...\")\n",
        "dataset.apply_model(\n",
        "    model, \n",
        "    label_field=\"isaac_detections\", \n",
        "    prompt_field=\"sample_level_objects\"\n",
        ")\n",
        "\n",
        "# Display detection results\n",
        "first_sample = dataset.first()\n",
        "if first_sample.isaac_detections and first_sample.isaac_detections.detections:\n",
        "    print(f\"Detected {len(first_sample.isaac_detections.detections)} objects in first image:\")\n",
        "    for det in first_sample.isaac_detections.detections[:5]:  # Show first 5\n",
        "        print(f\"  - {det.label}\")\n",
        "else:\n",
        "    print(\"No detections in first image\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Keypoint Detection\n",
        "\n",
        "Let's identify key points in the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to keypoint detection mode\n",
        "model.operation = \"point\"\n",
        "print(f\"System prompt for keypoints:\\n{model.system_prompt[:200]}...\\n\")\n",
        "\n",
        "# Apply keypoint detection using sample-level prompts\n",
        "print(\"Running keypoint detection...\")\n",
        "dataset.limit(10).apply_model(\n",
        "    model, \n",
        "    label_field=\"isaac_keypoints\", \n",
        "    prompt_field=\"sample_level_objects\"\n",
        ")\n",
        "\n",
        "# Display keypoint results\n",
        "first_sample = dataset.first()\n",
        "if first_sample.isaac_keypoints and first_sample.isaac_keypoints.keypoints:\n",
        "    print(f\"Detected {len(first_sample.isaac_keypoints.keypoints)} keypoints in first image:\")\n",
        "    for kp in first_sample.isaac_keypoints.keypoints[:5]:  # Show first 5\n",
        "        print(f\"  - {kp.label}\")\n",
        "else:\n",
        "    print(\"No keypoints detected in first image\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Image Classification\n",
        "\n",
        "Let's classify the weather/environment in the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to classification mode\n",
        "model.operation = \"classify\"\n",
        "print(f\"System prompt for classification:\\n{model.system_prompt[:200]}...\\n\")\n",
        "\n",
        "# Set classification prompt\n",
        "model.prompt = \"Classify the weather/environment in this scene into exactly one of the following: sunny, rainy, snowy, cloudy, indoor\"\n",
        "\n",
        "# Apply classification\n",
        "print(\"Running classification...\")\n",
        "dataset.limit(10).apply_model(model, label_field=\"weather_classification\")\n",
        "\n",
        "# Display classification results\n",
        "first_sample = dataset.first()\n",
        "if first_sample.weather_classification and first_sample.weather_classification.classifications:\n",
        "    print(f\"Weather classification for first image:\")\n",
        "    for cls in first_sample.weather_classification.classifications:\n",
        "        print(f\"  - {cls.label}\")\n",
        "else:\n",
        "    print(\"No classification for first image\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Testing on Text Images (OCR)\n",
        "\n",
        "Now let's test Isaac-0.1's OCR capabilities on images containing text:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a dataset with text images\n",
        "print(\"Loading text dataset...\")\n",
        "text_dataset = fouh.load_from_hub(\n",
        "    \"Voxel51/Total-Text-Dataset\",\n",
        "    max_samples=20  # Using fewer samples for demo\n",
        ")\n",
        "\n",
        "print(f\"Loaded {len(text_dataset)} text samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 OCR Text Extraction\n",
        "\n",
        "Extract text content from images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to OCR mode for text extraction\n",
        "model.operation = \"ocr\"\n",
        "print(f\"System prompt for OCR:\\n{model.system_prompt[:200]}...\\n\")\n",
        "\n",
        "# Set OCR prompt\n",
        "model.prompt = \"Report all text visible in this image\"\n",
        "\n",
        "# Apply OCR to extract text\n",
        "print(\"Running OCR text extraction...\")\n",
        "text_dataset.limit(10).apply_model(model, label_field=\"extracted_text\")\n",
        "\n",
        "# Display results\n",
        "first_text_sample = text_dataset.first()\n",
        "if first_text_sample.extracted_text:\n",
        "    print(f\"Extracted text from first image:\\n{first_text_sample.extracted_text[:200]}...\")\n",
        "else:\n",
        "    print(\"No text extracted from first image\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 OCR Text Detection\n",
        "\n",
        "Detect text regions with bounding boxes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to OCR detection mode\n",
        "model.operation = \"ocr_detection\"\n",
        "print(f\"System prompt for OCR detection:\\n{model.system_prompt[:200]}...\\n\")\n",
        "\n",
        "# Set OCR detection prompt\n",
        "model.prompt = \"Detect all text regions in this image\"\n",
        "\n",
        "# Apply OCR detection\n",
        "print(\"Running OCR text detection...\")\n",
        "text_dataset.limit(10).apply_model(model, label_field=\"text_regions\")\n",
        "\n",
        "# Display detection results\n",
        "first_text_sample = text_dataset.first()\n",
        "if first_text_sample.text_regions and first_text_sample.text_regions.detections:\n",
        "    print(f\"Detected {len(first_text_sample.text_regions.detections)} text regions in first image:\")\n",
        "    for det in first_text_sample.text_regions.detections[:5]:  # Show first 5\n",
        "        print(f\"  - '{det.label}'\")\n",
        "else:\n",
        "    print(\"No text regions detected in first image\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we demonstrated how to use Isaac-0.1 with FiftyOne for various computer vision tasks:\n",
        "\n",
        "1. **Visual Question Answering (VQA)** - Generated descriptions of spatial relationships\n",
        "2. **Object Detection** - Detected objects with bounding boxes\n",
        "3. **Keypoint Detection** - Identified key points in images\n",
        "4. **Classification** - Classified weather/environment conditions\n",
        "5. **OCR Text Extraction** - Extracted text content from images\n",
        "6. **OCR Text Detection** - Detected text regions with bounding boxes\n",
        "\n",
        "Isaac-0.1 is a powerful 2B-parameter model that delivers impressive results across all these tasks while being efficient enough for practical applications.\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Isaac-0.1 on Hugging Face](https://huggingface.co/PerceptronAI/Isaac-0.1)\n",
        "- [Isaac-0.1 FiftyOne Integration](https://github.com/harpreetsahota204/isaac0_1)\n",
        "- [Perceptron AI GitHub](https://github.com/perceptron-ai-inc/perceptron)\n",
        "- [FiftyOne Documentation](https://docs.voxel51.com/)\n",
        "\n",
        "## License\n",
        "\n",
        "- **Code**: Apache 2.0 License\n",
        "- **Model Weights**: Creative Commons Attribution-NonCommercial 4.0 International License\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
